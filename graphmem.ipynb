{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "graphmem.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohit-Mithra/ceiling-graph/blob/main/graphmem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U31K_Bgx60jC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51053f61-75c9-401b-d993-d638d1f9c99c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pwd\n",
        "%cd drive/My Drive/ceiling-graphs/TGN_ablation/TGN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content\n",
            "/content/drive/My Drive/ceiling-graphs/TGN_ablation/TGN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KLJmP1OsqnG",
        "outputId": "e50ebfe5-ac8d-4576-c0af-cff5f806ea9b"
      },
      "source": [
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "!python -c \"import torch; print(torch.__version__)\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.1\n",
            "1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAIfRD5Hs2zg"
      },
      "source": [
        "%%capture\n",
        "!pip install --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!pip install --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!pip install --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!pip install --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!!pip install git+https://github.com/rusty1s/pytorch_geometric.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzHmwxfRtIlS"
      },
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "from torch.nn import Linear, LSTM\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score\n",
        "\n",
        "from torch_geometric.datasets import JODIEDataset\n",
        "from torch_geometric.nn import TGNMemory, TransformerConv\n",
        "from torch_geometric.nn.models.tgn import (LastNeighborLoader, IdentityMessage, LastAggregator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRFX-QbhlKI1"
      },
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import pandas\n",
        "from torch_geometric.data import InMemoryDataset, TemporalData, download_url\n",
        "\n",
        "class genericDataset(InMemoryDataset):\n",
        "\n",
        "    def __init__(self, root, name, transform=None, pre_transform=None):\n",
        "        self.name = name.lower()\n",
        "        assert self.name == 'amazon'\n",
        "\n",
        "        super(genericDataset, self).__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_dir(self):\n",
        "        return osp.join(self.root, self.name, 'raw')\n",
        "\n",
        "    @property\n",
        "    def processed_dir(self):\n",
        "        return osp.join(self.root, self.name, 'processed')\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return f'{self.name}.csv'\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return 'data.pt'\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "        df = pandas.read_csv(self.raw_paths[0], skiprows=1, header=None)\n",
        "\n",
        "        src = torch.from_numpy(df.iloc[:, 0].values).to(torch.long)\n",
        "        dst = torch.from_numpy(df.iloc[:, 1].values).to(torch.long)\n",
        "        dst += int(src.max()) + 1\n",
        "        t = torch.from_numpy(df.iloc[:, 2].values).to(torch.long)\n",
        "        y = torch.from_numpy(df.iloc[:, 3].values).to(torch.long)\n",
        "        msg = torch.from_numpy(df.iloc[:, 4:].values).to(torch.float)\n",
        "\n",
        "        data = TemporalData(src=src, dst=dst, t=t, msg=msg, y=y)\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            data = self.pre_transform(data)\n",
        "\n",
        "        torch.save(self.collate([data]), self.processed_paths[0])\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.name.capitalize()}()'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "path = osp.join('..', 'data', 'JODIE')\n",
        "dataset = genericDataset(path, name='amazon')\n",
        "data = dataset[0].to(device)\n",
        "\n",
        "# Ensure to only sample actual destination nodes as negatives.\n",
        "min_dst_idx, max_dst_idx = int(data.dst.min()), int(data.dst.max())\n",
        "\n",
        "train_data, val_data, test_data = data.train_val_test_split(\n",
        "    val_ratio=0.15, test_ratio=0.15)\n",
        "\n",
        "neighbor_loader = LastNeighborLoader(data.num_nodes, size=10, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaDhC0v8ZTDF"
      },
      "source": [
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# path = osp.join('..', 'data', 'JODIE')\n",
        "# dataset = JODIEDataset(path, name='wikipedia')\n",
        "# data = dataset[0].to(device)\n",
        "\n",
        "# min_dst_idx, max_dst_idx = int(data.dst.min()), int(data.dst.max())\n",
        "\n",
        "# train_data, val_data, test_data = data.train_val_test_split(\n",
        "#     val_ratio=0.15, test_ratio=0.15)\n",
        "\n",
        "# neighbor_loader = LastNeighborLoader(data.num_nodes, size=10, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTI8Zw1H9cBH"
      },
      "source": [
        "from torch_geometric.nn.inits import zeros\n",
        "\n",
        "class global_memory(torch.nn.Module):\n",
        "    def __init__(self, in_channel, global_mem_dimension):\n",
        "        super(global_memory, self).__init__() \n",
        "        self.lin_global1 = Linear(in_channel*2, in_channel*2)\n",
        "        self.lin_global2 = Linear(in_channel*2, global_mem_dimension)\n",
        "\n",
        "    def forward(self, src_embedding, dst_embedding):\n",
        "        inp = torch.cat((src_embedding, dst_embedding), dim=1)\n",
        "        global_embedding = self.lin_global1(inp)\n",
        "        global_embedding = self.lin_global2(global_embedding)\n",
        "        # print('Global embedding shape')\n",
        "        # print(global_embedding.size())\n",
        "        return global_embedding\n",
        "\n",
        "\n",
        "class global_memory_lstm(torch.nn.Module):\n",
        "    def __init__(self, in_channel, hidden_dim, embedding_size):\n",
        "        super(global_memory_lstm, self).__init__() \n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.h = torch.zeros(1, 1, hidden_dim).cuda()\n",
        "        self.c = torch.zeros(1, 1, hidden_dim).cuda()\n",
        "        self.lstm_out = torch.zeros(200, 1, hidden_dim).cuda()\n",
        "        self.lstm = LSTM(in_channel*2, hidden_dim)\n",
        "        self.linear = Linear(hidden_dim, embedding_size)\n",
        "\n",
        "    def forward(self, src_embedding, dst_embedding):\n",
        "        inp = torch.cat((src_embedding, dst_embedding), dim=1)\n",
        "        lstm_out, (self.h, self.c) = self.lstm(inp.view(len(inp), 1, -1), (self.h, self.c))\n",
        "        # print(lstm_out.size())\n",
        "        embedding = self.linear(lstm_out.view(len(inp), -1))\n",
        "        return embedding\n",
        "\n",
        "    def reset_state(self):\n",
        "        zeros(self.h)\n",
        "        zeros(self.c)\n",
        "        zeros(self.lstm_out)\n",
        "\n",
        "    def detach(self):\n",
        "        self.h.detach_()\n",
        "        self.c.detach_()\n",
        "        self.lstm_out.detach_()\n",
        "\n",
        "\n",
        "class global_lstmcell(torch.nn.Module):\n",
        "    def __init__(self, in_channel, hidden_dim, embedding_size):\n",
        "        super(global_lstmcell, self).__init__() \n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.h = torch.zeros(1, hidden_dim).cuda()\n",
        "        self.gru = torch.nn.GRUCell(in_channel*2, hidden_dim)\n",
        "        self.linear = Linear(hidden_dim, embedding_size)\n",
        "\n",
        "    def forward(self, src_embedding, dst_embedding):\n",
        "        inp = torch.cat((src_embedding, dst_embedding), dim=1)\n",
        "\n",
        "        output = torch.zeros(inp.size()[0], self.hidden_dim).cuda()\n",
        "\n",
        "        for i in range(inp.size()[0]):\n",
        "            self.h = self.gru(inp[i].view(1, -1), self.h)\n",
        "            output[i] = self.h\n",
        "\n",
        "        # print(self.h.size())\n",
        "        embedding = self.linear(output.view(len(inp), -1))\n",
        "        return embedding\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.gru.reset_parameters()\n",
        "        zeros(self.h)    \n",
        "\n",
        "    def detach(self):\n",
        "        self.h.detach_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9i61TYszVCa"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class GraphAttentionEmbedding(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, msg_dim, time_enc):\n",
        "        super(GraphAttentionEmbedding, self).__init__()\n",
        "        self.time_enc = time_enc\n",
        "        edge_dim = msg_dim + time_enc.out_channels\n",
        "        self.conv = TransformerConv(in_channels, out_channels // 2, heads=2,\n",
        "                                    dropout=0.1, edge_dim=edge_dim)\n",
        "\n",
        "    def forward(self, x, last_update, edge_index, t, msg):\n",
        "        rel_t = last_update[edge_index[0]] - t\n",
        "        rel_t_enc = self.time_enc(rel_t.to(x.dtype))\n",
        "        edge_attr = torch.cat([rel_t_enc, msg], dim=-1)\n",
        "        return self.conv(x, edge_index, edge_attr)\n",
        "\n",
        "\n",
        "class LinkPredictor_global(torch.nn.Module):\n",
        "    def __init__(self, in_channels, global_in_channel):\n",
        "        super(LinkPredictor_global, self).__init__()\n",
        "        self.lin_src = Linear(in_channels+global_in_channel, in_channels+global_in_channel)\n",
        "        self.lin_dst = Linear(in_channels+global_in_channel, in_channels+global_in_channel)\n",
        "        self.lin_final = Linear(in_channels+global_in_channel, 1)\n",
        "\n",
        "    def forward(self, z_src, z_dst, z_global):\n",
        "        h = self.lin_src(torch.cat((z_src, z_global), dim=1)) + self.lin_dst(torch.cat((z_dst, z_global), dim=1))\n",
        "        # print(h.size())\n",
        "        h = h.relu()\n",
        "        return self.lin_final(h)\n",
        "\n",
        "class LinkPredictor(torch.nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(LinkPredictor, self).__init__()\n",
        "        self.lin_src = Linear(in_channels, in_channels)\n",
        "        self.lin_dst = Linear(in_channels, in_channels)\n",
        "        self.lin_final = Linear(in_channels, 1)\n",
        "\n",
        "    def forward(self, z_src, z_dst):\n",
        "        h = self.lin_src(z_src) + self.lin_dst(z_dst)\n",
        "        h = h.relu()\n",
        "        return self.lin_final(h)\n",
        "\n",
        "\n",
        "memory_dim = time_dim = embedding_dim = 100\n",
        "global_mem_dim = 100\n",
        "hidden_dim = 100\n",
        "\n",
        "memory = TGNMemory(\n",
        "    data.num_nodes,\n",
        "    data.msg.size(-1),\n",
        "    memory_dim,\n",
        "    time_dim,\n",
        "    message_module=IdentityMessage(data.msg.size(-1), memory_dim, time_dim),\n",
        "    aggregator_module=LastAggregator(),\n",
        ").to(device)\n",
        "\n",
        "gnn = GraphAttentionEmbedding(\n",
        "    in_channels=memory_dim,\n",
        "    out_channels=embedding_dim,\n",
        "    msg_dim=data.msg.size(-1),\n",
        "    time_enc=memory.time_enc,\n",
        ").to(device)\n",
        "\n",
        "#global_mem = global_memory(embedding_dim, global_mem_dim).to(device)\n",
        "# global_mem = global_memory_lstm(embedding_dim, global_mem_dim, global_mem_dim).to(device)\n",
        "global_mem = global_lstmcell(embedding_dim, hidden_dim, global_mem_dim).to(device)\n",
        "\n",
        "\n",
        "\n",
        "# link_pred = LinkPredictor(in_channels=embedding_dim).to(device)\n",
        "link_pred = LinkPredictor_global(in_channels=embedding_dim, global_in_channel = global_mem_dim).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    set(memory.parameters()) | set(gnn.parameters()) | set(global_mem.parameters())\n",
        "    | set(link_pred.parameters()), lr=0.0001)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "assoc = torch.empty(data.num_nodes, dtype=torch.long, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxzBopw-1k2f"
      },
      "source": [
        "def train():\n",
        "    memory.train()\n",
        "    gnn.train()\n",
        "    link_pred.train()\n",
        "    global_mem.train()\n",
        "\n",
        "    memory.reset_state()  # Start with a fresh memory.\n",
        "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
        "    global_mem.reset_state() #start with a fresh graph memory\n",
        "\n",
        "    total_loss = 0\n",
        "    total_global_loss = 0  #????\n",
        "    for batch in train_data.seq_batches(batch_size=200):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n",
        "\n",
        "        # Sample negative destination nodes.\n",
        "        neg_dst = torch.randint(min_dst_idx, max_dst_idx + 1, (src.size(0), ),\n",
        "                                dtype=torch.long, device=device)\n",
        "\n",
        "        n_id = torch.cat([src, pos_dst, neg_dst]).unique()\n",
        "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
        "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
        "\n",
        "        # Get updated memory of all nodes involved in the computation.\n",
        "        z, last_update = memory(n_id)\n",
        "        z = gnn(z, last_update, edge_index, data.t[e_id], data.msg[e_id])\n",
        "\n",
        "        z_global_pos = global_mem(z[assoc[src]], z[assoc[pos_dst]])\n",
        "        z_global_neg = global_mem(z[assoc[src]], z[assoc[neg_dst]])\n",
        "\n",
        "        # pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])\n",
        "        # neg_out = link_pred(z[assoc[src]], z[assoc[neg_dst]])\n",
        "\n",
        "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]], z_global_pos)\n",
        "        neg_out = link_pred(z[assoc[src]], z[assoc[neg_dst]], z_global_neg)\n",
        "\n",
        "        loss = criterion(pos_out, torch.ones_like(pos_out))\n",
        "        loss += criterion(neg_out, torch.zeros_like(neg_out))\n",
        "\n",
        "        # Update memory and neighbor loader with ground-truth state.\n",
        "        memory.update_state(src, pos_dst, t, msg)\n",
        "        neighbor_loader.insert(src, pos_dst)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        memory.detach()\n",
        "        global_mem.detach()\n",
        "        \n",
        "        total_loss += float(loss) * batch.num_events\n",
        "    return total_loss / train_data.num_events"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOcF4eRp1pMa"
      },
      "source": [
        "@torch.no_grad()\n",
        "def test(inference_data):\n",
        "    memory.eval()\n",
        "    gnn.eval()\n",
        "    link_pred.eval()\n",
        "    global_mem.eval()\n",
        "\n",
        "    torch.manual_seed(12345)  # Ensure deterministic sampling across epochs.\n",
        "\n",
        "    aps, aucs, acc = [], [], []\n",
        "    for batch in inference_data.seq_batches(batch_size=200):\n",
        "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n",
        "\n",
        "        neg_dst = torch.randint(min_dst_idx, max_dst_idx + 1, (src.size(0), ),\n",
        "                                dtype=torch.long, device=device)\n",
        "\n",
        "        n_id = torch.cat([src, pos_dst, neg_dst]).unique()\n",
        "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
        "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
        "\n",
        "\n",
        "        z, last_update = memory(n_id)\n",
        "        z = gnn(z, last_update, edge_index, data.t[e_id], data.msg[e_id])\n",
        "\n",
        "        z_global_pos = global_mem(z[assoc[src]], z[assoc[pos_dst]])\n",
        "        z_global_neg = global_mem(z[assoc[src]], z[assoc[neg_dst]])\n",
        " \n",
        "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]], z_global_pos)\n",
        "        neg_out = link_pred(z[assoc[src]], z[assoc[neg_dst]], z_global_neg)\n",
        "\n",
        "        y_pred = torch.cat([pos_out, neg_out], dim=0).sigmoid().cpu()\n",
        "        y_true = torch.cat(\n",
        "            [torch.ones(pos_out.size(0)),\n",
        "             torch.zeros(neg_out.size(0))], dim=0)\n",
        "\n",
        "        aps.append(average_precision_score(y_true, y_pred))\n",
        "        aucs.append(roc_auc_score(y_true, y_pred))\n",
        "        acc.append(accuracy_score(y_true, y_pred.round()))\n",
        "\n",
        "        memory.update_state(src, pos_dst, t, msg)\n",
        "        neighbor_loader.insert(src, pos_dst)\n",
        "        \n",
        "    return float(torch.tensor(aps).mean()), float(torch.tensor(aucs).mean()), float(torch.tensor(acc).mean())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "NjF3wwOb1wgH",
        "outputId": "acd94834-4d18-46c2-a565-e154da0c4f18"
      },
      "source": [
        "val_ap_list = []\n",
        "test_ap_list = []\n",
        "val_acc_list = []\n",
        "test_acc_list = []\n",
        "for epoch in range(1, 20):\n",
        "    loss = train()\n",
        "    print(f'  Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
        "    val_ap, val_auc, val_acc = test(val_data)\n",
        "    test_ap, test_auc, test_acc = test(test_data)\n",
        "    val_ap_list.append(val_ap)\n",
        "    test_ap_list.append(test_ap)\n",
        "    print(f' Val AP: {val_ap:.4f},  Val AUC: {val_auc:.4f}, Val Acc: {val_acc:.4f} ')\n",
        "    print(f'Test AP: {test_ap:.4f}, Test AUC: {test_auc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Epoch: 01, Loss: 0.6795\n",
            " Val AP: 0.9897,  Val AUC: 0.9897, Val Acc: 0.9425 \n",
            "Test AP: 0.9891, Test AUC: 0.9893, Test Acc: 0.9289\n",
            "  Epoch: 02, Loss: 0.6282\n",
            " Val AP: 0.9932,  Val AUC: 0.9934, Val Acc: 0.9600 \n",
            "Test AP: 0.9923, Test AUC: 0.9925, Test Acc: 0.9477\n",
            "  Epoch: 03, Loss: 0.5129\n",
            " Val AP: 0.9964,  Val AUC: 0.9966, Val Acc: 0.9731 \n",
            "Test AP: 0.9943, Test AUC: 0.9945, Test Acc: 0.9599\n",
            "  Epoch: 04, Loss: 0.4732\n",
            " Val AP: 0.9970,  Val AUC: 0.9973, Val Acc: 0.9782 \n",
            "Test AP: 0.9959, Test AUC: 0.9962, Test Acc: 0.9634\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e3b733b93873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_acc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'  Epoch: {epoch:02d}, Loss: {loss:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mval_ap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-1199cfe5e9e6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mneighbor_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__hKwLjoZceb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(5, 3))\n",
        "axes[0].plot(val_ap_list)\n",
        "axes[1].plot(test_ap_list)\n",
        "axes[0].set_ylim([0, 1.1])\n",
        "plt.ylim(ymin=0, ymax=1.1)\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W36ksC2G3LW1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.plot(val_ap_list)\n",
        "plt.ylabel('some numbers')\n",
        "plt.ylim(ymin=0, ymax=1.1)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wSdcTNfYX2a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}