{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "rel_pred_vanilla_kg.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzY4rBeOGCem",
        "outputId": "207c1491-6f8e-45d1-b997-5e309b0f3f58"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAIfRD5Hs2zg"
      },
      "source": [
        "%%capture\n",
        "! pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "! pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "! pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "! pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "! pip install git+https://github.com/rusty1s/pytorch_geometric.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzHmwxfRtIlS"
      },
      "source": [
        "import os.path as osp\n",
        "import torch\n",
        "from torch.nn import Linear, LSTM\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score\n",
        "from torch_geometric.datasets import JODIEDataset\n",
        "from torch_geometric.nn import TGNMemory, TransformerConv\n",
        "from torch_geometric.nn.models.tgn import (LastNeighborLoader, IdentityMessage, LastAggregator)\n",
        "from torch_geometric.data import InMemoryDataset, TemporalData, download_url\n",
        "from torch_geometric.nn.inits import zeros\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.nn import Parameter\n",
        "from attr import attrs, attrib, Factory\n",
        "import warnings\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import os.path as osp\n",
        "import torch\n",
        "import pandas\n",
        "from torch_geometric.data import InMemoryDataset, TemporalData, download_url\n",
        "\n",
        "class genericDataset(InMemoryDataset):\n",
        "\n",
        "    def __init__(self, root, name, transform=None, pre_transform=None):\n",
        "        self.name = name.lower()\n",
        "\n",
        "        super(genericDataset, self).__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_dir(self):\n",
        "        return osp.join(self.root, self.name, 'raw')\n",
        "\n",
        "    @property\n",
        "    def processed_dir(self):\n",
        "        return osp.join(self.root, self.name, 'processed')\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return f'{self.name}.csv'\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return 'data.pt'\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "        df = pandas.read_csv(self.raw_paths[0], skiprows=1, header=None, index_col = None)\n",
        "        print(df.head())\n",
        "        src = torch.from_numpy(df.iloc[:, 0].values).to(torch.long)\n",
        "        dst = torch.from_numpy(df.iloc[:, 1].values).to(torch.long)\n",
        "        dst += int(src.max()) + 1\n",
        "        t = torch.from_numpy(df.iloc[:, 2].values).to(torch.long)\n",
        "        msg = torch.from_numpy(df.iloc[:, 4:].values).to(torch.long)\n",
        "        y = torch.from_numpy(df.iloc[:, 3].values).to(torch.long)\n",
        "        # msg = torch.from_numpy(df.iloc[:, 4:].values).to(torch.float)\n",
        "\n",
        "        data = TemporalData(src=src, dst=dst, t=t, msg=msg, y=y)\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            data = self.pre_transform(data)\n",
        "\n",
        "        torch.save(self.collate([data]), self.processed_paths[0])\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.name.capitalize()}()'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "path = osp.join('/content/drive/MyDrive/ceiling-graphs/TGN_ablation/Knowledge Graph', 'Data', 'JODIE')\n",
        "dataset = genericDataset(path, name='wiki_large')\n",
        "data = dataset[0].to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9i61TYszVCa"
      },
      "source": [
        "# Ensure to only sample actual destination nodes as negatives.\n",
        "min_dst_idx, max_dst_idx = int(data.dst.min()), int(data.dst.max())\n",
        "\n",
        "train_data, val_data, test_data = data.train_val_test_split(\n",
        "    val_ratio=0.15, test_ratio=0.15)\n",
        "\n",
        "neighbor_loader = LastNeighborLoader(data.num_nodes, size=10, device=device)\n",
        "\n",
        "class GraphAttentionEmbedding(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, msg_dim, time_enc):\n",
        "        super(GraphAttentionEmbedding, self).__init__()\n",
        "        self.time_enc = time_enc\n",
        "        edge_dim = msg_dim + time_enc.out_channels\n",
        "        self.conv = TransformerConv(in_channels, out_channels // 2, heads=2,\n",
        "                                    dropout=0.1, edge_dim=edge_dim)\n",
        "\n",
        "    def forward(self, x, last_update, edge_index, t, msg):\n",
        "        rel_t = last_update[edge_index[0]] - t\n",
        "        rel_t_enc = self.time_enc(rel_t.to(x.dtype))\n",
        "        edge_attr = torch.cat([rel_t_enc, msg], dim=-1)\n",
        "        return self.conv(x, edge_index, edge_attr)\n",
        "\n",
        "class LinkPredictor(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(LinkPredictor, self).__init__()\n",
        "        self.lin_src = Linear(in_channels, in_channels)\n",
        "        self.lin_dst = Linear(in_channels, in_channels)\n",
        "        self.lin_final = Linear(in_channels, out_channels)\n",
        "        self.m = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, z_src, z_dst):\n",
        "        h = self.lin_src(z_src) + self.lin_dst(z_dst)\n",
        "        h = h.relu()\n",
        "        return self.m(self.lin_final(h))\n",
        "\n",
        "memory_dim = time_dim = embedding_dim = 100\n",
        "\n",
        "memory = TGNMemory(\n",
        "    data.num_nodes,\n",
        "    data.msg.size(-1),\n",
        "    memory_dim,\n",
        "    time_dim,\n",
        "    message_module=IdentityMessage(data.msg.size(-1), memory_dim, time_dim),\n",
        "    aggregator_module=LastAggregator(),\n",
        ").to(device)\n",
        "\n",
        "gnn = GraphAttentionEmbedding(\n",
        "    in_channels=memory_dim,\n",
        "    out_channels=embedding_dim,\n",
        "    msg_dim=data.msg.size(-1),\n",
        "    time_enc=memory.time_enc,\n",
        ").to(device)\n",
        "\n",
        "link_pred = LinkPredictor(in_channels=embedding_dim, out_channels = 25).to(device)  # ********************Change here\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    set(memory.parameters()) | set(gnn.parameters())\n",
        "    | set(link_pred.parameters()), lr=0.0001)\n",
        "\n",
        "criterion = torch.nn.NLLLoss()\n",
        "\n",
        "# Helper vector to map global node indices to local ones.\n",
        "assoc = torch.empty(data.num_nodes, dtype=torch.long, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxzBopw-1k2f"
      },
      "source": [
        "def metrics(logits, y):\n",
        "\n",
        "    _, perm = logits.sort(dim=1, descending=True)\n",
        "    mask = (y.view(-1, 1) == perm)\n",
        "\n",
        "    nnz = mask.nonzero(as_tuple=False)\n",
        "    mr = (nnz[:, -1] + 1).to(torch.float).mean().item()\n",
        "    mrr = (1 / (nnz[:, -1] + 1).to(torch.float)).mean().item()\n",
        "    hits1 = mask[:, :1].sum().item() / y.size(0)\n",
        "    hits3 = mask[:, :3].sum().item() / y.size(0)\n",
        "    # hits10 = mask[:, :10].sum().item() / y.size(0)\n",
        "\n",
        "    return torch.tensor([mrr, mr, hits1, hits3])\n",
        "\n",
        "\n",
        "def train():\n",
        "    memory.train()\n",
        "    gnn.train()\n",
        "    link_pred.train()\n",
        "\n",
        "    memory.reset_state()  # Start with a fresh memory.\n",
        "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
        "\n",
        "    total_loss = 0\n",
        "    acc = []\n",
        "    for batch in train_data.seq_batches(batch_size=200):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        src, dst, t, msg, labels = batch.src, batch.dst, batch.t, batch.msg, batch.y\n",
        "\n",
        "        n_id = torch.cat([src, dst]).unique()\n",
        "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
        "\n",
        "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
        "\n",
        "        z, last_update = memory(n_id)\n",
        "        z = gnn(z, last_update, edge_index, data.t[e_id], data.msg[e_id])\n",
        "\n",
        "        log_prob_rel = link_pred(z[assoc[src]], z[assoc[dst]])\n",
        "\n",
        "        loss = criterion(log_prob_rel, labels)\n",
        "\n",
        "        memory.update_state(src, dst, t, msg)\n",
        "        neighbor_loader.insert(src, dst)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        memory.detach()\n",
        "\n",
        "        total_loss += float(loss) * batch.num_events\n",
        "        y_pred = torch.argmax(log_prob_rel, dim=1)\n",
        "        acc.append(accuracy_score(labels.cpu(), y_pred.cpu()))\n",
        "\n",
        "    return total_loss / train_data.num_events, float(torch.tensor(acc).mean())\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(inference_data):\n",
        "    memory.eval()\n",
        "    gnn.eval()\n",
        "    link_pred.eval()\n",
        "\n",
        "    acc = []\n",
        "    result = torch.tensor([0, 0, 0, 0], dtype=torch.float)\n",
        "    for batch in inference_data.seq_batches(batch_size=200):\n",
        "\n",
        "        src, dst, t, msg, label = batch.src, batch.dst, batch.t, batch.msg, batch.y\n",
        "    \n",
        "        n_id = torch.cat([src, dst]).unique()\n",
        "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
        "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
        "\n",
        "        z, last_update = memory(n_id)\n",
        "        z = gnn(z, last_update, edge_index, data.t[e_id], data.msg[e_id])\n",
        "\n",
        "        log_prob_rel = link_pred(z[assoc[src]], z[assoc[dst]])\n",
        "\n",
        "        result += metrics(log_prob_rel, label) * src.size(0)\n",
        "        \n",
        "        memory.update_state(src, dst, t, msg)\n",
        "        neighbor_loader.insert(src, dst)\n",
        "\n",
        "        y_pred = torch.argmax(log_prob_rel, dim=1)\n",
        "                \n",
        "        acc.append(accuracy_score(label.cpu(), y_pred.cpu()))\n",
        "\n",
        "    result = result / (inference_data.num_events)\n",
        "    return float(torch.tensor(acc).mean()), result.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjF3wwOb1wgH",
        "outputId": "2691c8f2-1f9b-4ecb-f0e1-4b5b08170eaf"
      },
      "source": [
        "for epoch in range(1, 100):\n",
        "    loss, train_acc = train()\n",
        "    print(f'  Epoch: {epoch:02d}, Loss: {loss:.4f}, Acc: {train_acc:.4f}')\n",
        "    val_acc, res = test(val_data)\n",
        "    print('Epoch: {:02d}, MRR: {:.4f}, MR: {:.4f}, Hits@1: {:.4f}, Hits@3: {:.4f}'.format(epoch, *res))\n",
        "    test_acc, res = test(test_data)\n",
        "    print('Epoch: {:02d}, MRR: {:.4f}, MR: {:.4f}, Hits@1: {:.4f}, Hits@3: {:.4f}'.format(epoch, *res))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Epoch: 01, Loss: 2.6416, Acc: 0.2515\n",
            "Epoch: 01, MRR: 0.7651, MR: 2.1428, Hits@1: 0.6426, Hits@3: 0.8830\n",
            "Epoch: 01, MRR: 0.6828, MR: 2.8621, Hits@1: 0.5546, Hits@3: 0.7542\n",
            "\n",
            "  Epoch: 02, Loss: 2.1021, Acc: 0.3298\n",
            "Epoch: 02, MRR: 0.7732, MR: 2.0541, Hits@1: 0.6501, Hits@3: 0.8841\n",
            "Epoch: 02, MRR: 0.6824, MR: 2.8317, Hits@1: 0.5572, Hits@3: 0.7554\n",
            "\n",
            "  Epoch: 03, Loss: 1.9861, Acc: 0.3447\n",
            "Epoch: 03, MRR: 0.7695, MR: 2.0461, Hits@1: 0.6423, Hits@3: 0.8852\n",
            "Epoch: 03, MRR: 0.6840, MR: 2.7664, Hits@1: 0.5545, Hits@3: 0.7586\n",
            "\n",
            "  Epoch: 04, Loss: 1.9228, Acc: 0.3439\n",
            "Epoch: 04, MRR: 0.7696, MR: 2.0532, Hits@1: 0.6435, Hits@3: 0.8873\n",
            "Epoch: 04, MRR: 0.6870, MR: 2.7432, Hits@1: 0.5555, Hits@3: 0.7836\n",
            "\n",
            "  Epoch: 05, Loss: 1.8615, Acc: 0.3546\n",
            "Epoch: 05, MRR: 0.7720, MR: 2.0386, Hits@1: 0.6469, Hits@3: 0.8890\n",
            "Epoch: 05, MRR: 0.6945, MR: 2.6925, Hits@1: 0.5596, Hits@3: 0.8018\n",
            "\n",
            "  Epoch: 06, Loss: 1.7969, Acc: 0.3873\n",
            "Epoch: 06, MRR: 0.7833, MR: 1.9937, Hits@1: 0.6647, Hits@3: 0.8953\n",
            "Epoch: 06, MRR: 0.7077, MR: 2.6305, Hits@1: 0.5762, Hits@3: 0.8095\n",
            "\n",
            "  Epoch: 07, Loss: 1.7415, Acc: 0.4185\n",
            "Epoch: 07, MRR: 0.8033, MR: 1.9187, Hits@1: 0.6969, Hits@3: 0.9104\n",
            "Epoch: 07, MRR: 0.7200, MR: 2.6099, Hits@1: 0.5926, Hits@3: 0.8186\n",
            "\n",
            "  Epoch: 08, Loss: 1.6793, Acc: 0.4400\n",
            "Epoch: 08, MRR: 0.8115, MR: 1.8796, Hits@1: 0.7086, Hits@3: 0.9096\n",
            "Epoch: 08, MRR: 0.7036, MR: 2.5861, Hits@1: 0.5524, Hits@3: 0.8232\n",
            "\n",
            "  Epoch: 09, Loss: 1.6387, Acc: 0.4533\n",
            "Epoch: 09, MRR: 0.8172, MR: 1.8608, Hits@1: 0.7163, Hits@3: 0.9170\n",
            "Epoch: 09, MRR: 0.7234, MR: 2.5371, Hits@1: 0.5895, Hits@3: 0.8276\n",
            "\n",
            "  Epoch: 10, Loss: 1.5912, Acc: 0.4663\n",
            "Epoch: 10, MRR: 0.8228, MR: 1.8342, Hits@1: 0.7246, Hits@3: 0.9226\n",
            "Epoch: 10, MRR: 0.7471, MR: 2.4562, Hits@1: 0.6320, Hits@3: 0.8322\n",
            "\n",
            "  Epoch: 11, Loss: 1.5602, Acc: 0.4774\n",
            "Epoch: 11, MRR: 0.8309, MR: 1.8159, Hits@1: 0.7373, Hits@3: 0.9184\n",
            "Epoch: 11, MRR: 0.7486, MR: 2.4450, Hits@1: 0.6317, Hits@3: 0.8300\n",
            "\n",
            "  Epoch: 12, Loss: 1.5456, Acc: 0.4803\n",
            "Epoch: 12, MRR: 0.8277, MR: 1.8614, Hits@1: 0.7338, Hits@3: 0.9118\n",
            "Epoch: 12, MRR: 0.7497, MR: 2.4533, Hits@1: 0.6361, Hits@3: 0.8273\n",
            "\n",
            "  Epoch: 13, Loss: 1.5228, Acc: 0.4922\n",
            "Epoch: 13, MRR: 0.8335, MR: 1.8684, Hits@1: 0.7433, Hits@3: 0.9133\n",
            "Epoch: 13, MRR: 0.7527, MR: 2.4486, Hits@1: 0.6396, Hits@3: 0.8300\n",
            "\n",
            "  Epoch: 14, Loss: 1.4745, Acc: 0.5080\n",
            "Epoch: 14, MRR: 0.8418, MR: 1.7773, Hits@1: 0.7557, Hits@3: 0.9169\n",
            "Epoch: 14, MRR: 0.7553, MR: 2.3864, Hits@1: 0.6415, Hits@3: 0.8331\n",
            "\n",
            "  Epoch: 15, Loss: 1.4364, Acc: 0.5293\n",
            "Epoch: 15, MRR: 0.8460, MR: 1.7800, Hits@1: 0.7622, Hits@3: 0.9128\n",
            "Epoch: 15, MRR: 0.7640, MR: 2.3689, Hits@1: 0.6565, Hits@3: 0.8390\n",
            "\n",
            "  Epoch: 16, Loss: 1.4073, Acc: 0.5476\n",
            "Epoch: 16, MRR: 0.8508, MR: 1.7654, Hits@1: 0.7671, Hits@3: 0.9264\n",
            "Epoch: 16, MRR: 0.7687, MR: 2.3201, Hits@1: 0.6638, Hits@3: 0.8470\n",
            "\n",
            "  Epoch: 17, Loss: 1.3869, Acc: 0.5472\n",
            "Epoch: 17, MRR: 0.8544, MR: 1.7368, Hits@1: 0.7718, Hits@3: 0.9308\n",
            "Epoch: 17, MRR: 0.7735, MR: 2.3299, Hits@1: 0.6724, Hits@3: 0.8487\n",
            "\n",
            "  Epoch: 18, Loss: 1.3598, Acc: 0.5683\n",
            "Epoch: 18, MRR: 0.8623, MR: 1.6989, Hits@1: 0.7837, Hits@3: 0.9347\n",
            "Epoch: 18, MRR: 0.7831, MR: 2.2480, Hits@1: 0.6855, Hits@3: 0.8553\n",
            "\n",
            "  Epoch: 19, Loss: 1.3420, Acc: 0.5572\n",
            "Epoch: 19, MRR: 0.8668, MR: 1.6729, Hits@1: 0.7900, Hits@3: 0.9335\n",
            "Epoch: 19, MRR: 0.7981, MR: 2.2303, Hits@1: 0.7131, Hits@3: 0.8576\n",
            "\n",
            "  Epoch: 20, Loss: 1.3301, Acc: 0.5725\n",
            "Epoch: 20, MRR: 0.8752, MR: 1.7010, Hits@1: 0.8062, Hits@3: 0.9420\n",
            "Epoch: 20, MRR: 0.7921, MR: 2.2466, Hits@1: 0.7024, Hits@3: 0.8584\n",
            "\n",
            "  Epoch: 21, Loss: 1.2962, Acc: 0.5779\n",
            "Epoch: 21, MRR: 0.8824, MR: 1.6044, Hits@1: 0.8132, Hits@3: 0.9456\n",
            "Epoch: 21, MRR: 0.8034, MR: 2.1632, Hits@1: 0.7134, Hits@3: 0.8647\n",
            "\n",
            "  Epoch: 22, Loss: 1.2660, Acc: 0.5924\n",
            "Epoch: 22, MRR: 0.8918, MR: 1.5816, Hits@1: 0.8295, Hits@3: 0.9483\n",
            "Epoch: 22, MRR: 0.8087, MR: 2.1474, Hits@1: 0.7221, Hits@3: 0.8678\n",
            "\n",
            "  Epoch: 23, Loss: 1.2598, Acc: 0.5928\n",
            "Epoch: 23, MRR: 0.8891, MR: 1.5838, Hits@1: 0.8235, Hits@3: 0.9481\n",
            "Epoch: 23, MRR: 0.8096, MR: 2.0841, Hits@1: 0.7206, Hits@3: 0.8671\n",
            "\n",
            "  Epoch: 24, Loss: 1.2544, Acc: 0.5881\n",
            "Epoch: 24, MRR: 0.8889, MR: 1.6726, Hits@1: 0.8266, Hits@3: 0.9415\n",
            "Epoch: 24, MRR: 0.8079, MR: 2.0969, Hits@1: 0.7224, Hits@3: 0.8678\n",
            "\n",
            "  Epoch: 25, Loss: 1.2307, Acc: 0.6007\n",
            "Epoch: 25, MRR: 0.8938, MR: 1.6355, Hits@1: 0.8348, Hits@3: 0.9446\n",
            "Epoch: 25, MRR: 0.8121, MR: 2.0547, Hits@1: 0.7260, Hits@3: 0.8683\n",
            "\n",
            "  Epoch: 26, Loss: 1.2065, Acc: 0.6075\n",
            "Epoch: 26, MRR: 0.8956, MR: 1.5444, Hits@1: 0.8348, Hits@3: 0.9483\n",
            "Epoch: 26, MRR: 0.8222, MR: 2.0071, Hits@1: 0.7435, Hits@3: 0.8689\n",
            "\n",
            "  Epoch: 27, Loss: 1.2152, Acc: 0.6033\n",
            "Epoch: 27, MRR: 0.8964, MR: 1.5434, Hits@1: 0.8363, Hits@3: 0.9502\n",
            "Epoch: 27, MRR: 0.8171, MR: 2.0124, Hits@1: 0.7345, Hits@3: 0.8695\n",
            "\n",
            "  Epoch: 28, Loss: 1.1924, Acc: 0.6121\n",
            "Epoch: 28, MRR: 0.8871, MR: 1.5869, Hits@1: 0.8234, Hits@3: 0.9436\n",
            "Epoch: 28, MRR: 0.8018, MR: 2.0743, Hits@1: 0.7122, Hits@3: 0.8587\n",
            "\n",
            "  Epoch: 29, Loss: 1.1695, Acc: 0.6240\n",
            "Epoch: 29, MRR: 0.8996, MR: 1.5367, Hits@1: 0.8409, Hits@3: 0.9490\n",
            "Epoch: 29, MRR: 0.8277, MR: 1.9612, Hits@1: 0.7501, Hits@3: 0.8713\n",
            "\n",
            "  Epoch: 30, Loss: 1.1731, Acc: 0.6243\n",
            "Epoch: 30, MRR: 0.8959, MR: 1.5367, Hits@1: 0.8337, Hits@3: 0.9493\n",
            "Epoch: 30, MRR: 0.8119, MR: 2.0236, Hits@1: 0.7309, Hits@3: 0.8577\n",
            "\n",
            "  Epoch: 31, Loss: 1.1520, Acc: 0.6323\n",
            "Epoch: 31, MRR: 0.8945, MR: 1.5381, Hits@1: 0.8310, Hits@3: 0.9510\n",
            "Epoch: 31, MRR: 0.8120, MR: 1.9835, Hits@1: 0.7231, Hits@3: 0.8688\n",
            "\n",
            "  Epoch: 32, Loss: 1.1652, Acc: 0.6273\n",
            "Epoch: 32, MRR: 0.8967, MR: 1.5301, Hits@1: 0.8329, Hits@3: 0.9531\n",
            "Epoch: 32, MRR: 0.8217, MR: 1.9342, Hits@1: 0.7387, Hits@3: 0.8715\n",
            "\n",
            "  Epoch: 33, Loss: 1.1558, Acc: 0.6324\n",
            "Epoch: 33, MRR: 0.8950, MR: 1.6034, Hits@1: 0.8341, Hits@3: 0.9495\n",
            "Epoch: 33, MRR: 0.8113, MR: 2.0447, Hits@1: 0.7294, Hits@3: 0.8584\n",
            "\n",
            "  Epoch: 34, Loss: 1.1397, Acc: 0.6376\n",
            "Epoch: 34, MRR: 0.8940, MR: 1.5843, Hits@1: 0.8322, Hits@3: 0.9488\n",
            "Epoch: 34, MRR: 0.8193, MR: 1.9740, Hits@1: 0.7367, Hits@3: 0.8701\n",
            "\n",
            "  Epoch: 35, Loss: 1.1370, Acc: 0.6348\n",
            "Epoch: 35, MRR: 0.8973, MR: 1.5289, Hits@1: 0.8368, Hits@3: 0.9522\n",
            "Epoch: 35, MRR: 0.8245, MR: 1.8734, Hits@1: 0.7421, Hits@3: 0.8654\n",
            "\n",
            "  Epoch: 36, Loss: 1.1400, Acc: 0.6358\n",
            "Epoch: 36, MRR: 0.8937, MR: 1.5417, Hits@1: 0.8331, Hits@3: 0.9461\n",
            "Epoch: 36, MRR: 0.8207, MR: 1.9592, Hits@1: 0.7396, Hits@3: 0.8659\n",
            "\n",
            "  Epoch: 37, Loss: 1.1186, Acc: 0.6455\n",
            "Epoch: 37, MRR: 0.8956, MR: 1.5340, Hits@1: 0.8351, Hits@3: 0.9476\n",
            "Epoch: 37, MRR: 0.8144, MR: 1.9118, Hits@1: 0.7253, Hits@3: 0.8664\n",
            "\n",
            "  Epoch: 38, Loss: 1.0978, Acc: 0.6516\n",
            "Epoch: 38, MRR: 0.9006, MR: 1.5126, Hits@1: 0.8412, Hits@3: 0.9521\n",
            "Epoch: 38, MRR: 0.8228, MR: 1.9604, Hits@1: 0.7450, Hits@3: 0.8659\n",
            "\n",
            "  Epoch: 39, Loss: 1.1087, Acc: 0.6471\n",
            "Epoch: 39, MRR: 0.8950, MR: 1.5388, Hits@1: 0.8336, Hits@3: 0.9487\n",
            "Epoch: 39, MRR: 0.8145, MR: 1.9223, Hits@1: 0.7297, Hits@3: 0.8610\n",
            "\n",
            "  Epoch: 40, Loss: 1.0783, Acc: 0.6558\n",
            "Epoch: 40, MRR: 0.8986, MR: 1.5109, Hits@1: 0.8378, Hits@3: 0.9539\n",
            "Epoch: 40, MRR: 0.8151, MR: 1.9150, Hits@1: 0.7280, Hits@3: 0.8655\n",
            "\n",
            "  Epoch: 41, Loss: 1.0769, Acc: 0.6570\n",
            "Epoch: 41, MRR: 0.9028, MR: 1.4973, Hits@1: 0.8448, Hits@3: 0.9534\n",
            "Epoch: 41, MRR: 0.8288, MR: 1.8021, Hits@1: 0.7460, Hits@3: 0.8689\n",
            "\n",
            "  Epoch: 42, Loss: 1.0831, Acc: 0.6542\n",
            "Epoch: 42, MRR: 0.8976, MR: 1.5213, Hits@1: 0.8371, Hits@3: 0.9510\n",
            "Epoch: 42, MRR: 0.8195, MR: 1.8742, Hits@1: 0.7365, Hits@3: 0.8608\n",
            "\n",
            "  Epoch: 43, Loss: 1.0743, Acc: 0.6586\n",
            "Epoch: 43, MRR: 0.9054, MR: 1.4940, Hits@1: 0.8497, Hits@3: 0.9532\n",
            "Epoch: 43, MRR: 0.8089, MR: 1.9305, Hits@1: 0.7148, Hits@3: 0.8676\n",
            "\n",
            "  Epoch: 44, Loss: 1.0560, Acc: 0.6610\n",
            "Epoch: 44, MRR: 0.9039, MR: 1.5048, Hits@1: 0.8472, Hits@3: 0.9534\n",
            "Epoch: 44, MRR: 0.8229, MR: 1.9276, Hits@1: 0.7450, Hits@3: 0.8604\n",
            "\n",
            "  Epoch: 45, Loss: 1.0544, Acc: 0.6669\n",
            "Epoch: 45, MRR: 0.9045, MR: 1.5053, Hits@1: 0.8487, Hits@3: 0.9527\n",
            "Epoch: 45, MRR: 0.8177, MR: 1.9550, Hits@1: 0.7379, Hits@3: 0.8542\n",
            "\n",
            "  Epoch: 46, Loss: 1.0351, Acc: 0.6644\n",
            "Epoch: 46, MRR: 0.9058, MR: 1.4760, Hits@1: 0.8487, Hits@3: 0.9563\n",
            "Epoch: 46, MRR: 0.8212, MR: 1.9291, Hits@1: 0.7438, Hits@3: 0.8582\n",
            "\n",
            "  Epoch: 47, Loss: 1.0634, Acc: 0.6564\n",
            "Epoch: 47, MRR: 0.9046, MR: 1.4876, Hits@1: 0.8470, Hits@3: 0.9546\n",
            "Epoch: 47, MRR: 0.8292, MR: 1.8460, Hits@1: 0.7500, Hits@3: 0.8706\n",
            "\n",
            "  Epoch: 48, Loss: 1.0189, Acc: 0.6746\n",
            "Epoch: 48, MRR: 0.9056, MR: 1.4956, Hits@1: 0.8477, Hits@3: 0.9558\n",
            "Epoch: 48, MRR: 0.8248, MR: 1.8819, Hits@1: 0.7457, Hits@3: 0.8594\n",
            "\n",
            "  Epoch: 49, Loss: 1.0086, Acc: 0.6795\n",
            "Epoch: 49, MRR: 0.9029, MR: 1.4881, Hits@1: 0.8433, Hits@3: 0.9566\n",
            "Epoch: 49, MRR: 0.8199, MR: 1.8655, Hits@1: 0.7360, Hits@3: 0.8570\n",
            "\n",
            "  Epoch: 50, Loss: 1.0194, Acc: 0.6745\n",
            "Epoch: 50, MRR: 0.9028, MR: 1.4988, Hits@1: 0.8443, Hits@3: 0.9553\n",
            "Epoch: 50, MRR: 0.8356, MR: 1.7943, Hits@1: 0.7596, Hits@3: 0.8627\n",
            "\n",
            "  Epoch: 51, Loss: 0.9968, Acc: 0.6842\n",
            "Epoch: 51, MRR: 0.9043, MR: 1.4793, Hits@1: 0.8456, Hits@3: 0.9556\n",
            "Epoch: 51, MRR: 0.8276, MR: 1.9033, Hits@1: 0.7542, Hits@3: 0.8494\n",
            "\n",
            "  Epoch: 52, Loss: 1.0152, Acc: 0.6747\n",
            "Epoch: 52, MRR: 0.9046, MR: 1.4995, Hits@1: 0.8472, Hits@3: 0.9534\n",
            "Epoch: 52, MRR: 0.8291, MR: 1.8169, Hits@1: 0.7496, Hits@3: 0.8645\n",
            "\n",
            "  Epoch: 53, Loss: 1.0111, Acc: 0.6733\n",
            "Epoch: 53, MRR: 0.9047, MR: 1.4940, Hits@1: 0.8480, Hits@3: 0.9556\n",
            "Epoch: 53, MRR: 0.8254, MR: 1.9130, Hits@1: 0.7513, Hits@3: 0.8547\n",
            "\n",
            "  Epoch: 54, Loss: 0.9779, Acc: 0.6862\n",
            "Epoch: 54, MRR: 0.9073, MR: 1.4840, Hits@1: 0.8519, Hits@3: 0.9553\n",
            "Epoch: 54, MRR: 0.8253, MR: 1.8944, Hits@1: 0.7506, Hits@3: 0.8470\n",
            "\n",
            "  Epoch: 55, Loss: 1.0016, Acc: 0.6787\n",
            "Epoch: 55, MRR: 0.9059, MR: 1.4796, Hits@1: 0.8484, Hits@3: 0.9573\n",
            "Epoch: 55, MRR: 0.8218, MR: 2.0413, Hits@1: 0.7506, Hits@3: 0.8513\n",
            "\n",
            "  Epoch: 56, Loss: 0.9838, Acc: 0.6860\n",
            "Epoch: 56, MRR: 0.9049, MR: 1.4929, Hits@1: 0.8477, Hits@3: 0.9560\n",
            "Epoch: 56, MRR: 0.8231, MR: 1.8968, Hits@1: 0.7404, Hits@3: 0.8661\n",
            "\n",
            "  Epoch: 57, Loss: 0.9894, Acc: 0.6863\n",
            "Epoch: 57, MRR: 0.9027, MR: 1.4973, Hits@1: 0.8434, Hits@3: 0.9558\n",
            "Epoch: 57, MRR: 0.8204, MR: 1.9114, Hits@1: 0.7387, Hits@3: 0.8672\n",
            "\n",
            "  Epoch: 58, Loss: 0.9893, Acc: 0.6821\n",
            "Epoch: 58, MRR: 0.9050, MR: 1.4731, Hits@1: 0.8456, Hits@3: 0.9594\n",
            "Epoch: 58, MRR: 0.8277, MR: 1.8321, Hits@1: 0.7457, Hits@3: 0.8701\n",
            "\n",
            "  Epoch: 59, Loss: 0.9776, Acc: 0.6868\n",
            "Epoch: 59, MRR: 0.9054, MR: 1.4815, Hits@1: 0.8467, Hits@3: 0.9589\n",
            "Epoch: 59, MRR: 0.8242, MR: 1.8807, Hits@1: 0.7440, Hits@3: 0.8610\n",
            "\n",
            "  Epoch: 60, Loss: 0.9584, Acc: 0.6955\n",
            "Epoch: 60, MRR: 0.9067, MR: 1.4713, Hits@1: 0.8489, Hits@3: 0.9592\n",
            "Epoch: 60, MRR: 0.8266, MR: 1.8134, Hits@1: 0.7404, Hits@3: 0.8793\n",
            "\n",
            "  Epoch: 61, Loss: 0.9571, Acc: 0.6949\n",
            "Epoch: 61, MRR: 0.9053, MR: 1.4793, Hits@1: 0.8470, Hits@3: 0.9575\n",
            "Epoch: 61, MRR: 0.8267, MR: 1.9504, Hits@1: 0.7517, Hits@3: 0.8621\n",
            "\n",
            "  Epoch: 62, Loss: 0.9517, Acc: 0.6951\n",
            "Epoch: 62, MRR: 0.9087, MR: 1.4679, Hits@1: 0.8518, Hits@3: 0.9607\n",
            "Epoch: 62, MRR: 0.8236, MR: 1.9895, Hits@1: 0.7520, Hits@3: 0.8496\n",
            "\n",
            "  Epoch: 63, Loss: 0.9607, Acc: 0.6939\n",
            "Epoch: 63, MRR: 0.9076, MR: 1.4650, Hits@1: 0.8495, Hits@3: 0.9587\n",
            "Epoch: 63, MRR: 0.8239, MR: 1.8817, Hits@1: 0.7440, Hits@3: 0.8553\n",
            "\n",
            "  Epoch: 64, Loss: 0.9607, Acc: 0.6882\n",
            "Epoch: 64, MRR: 0.9079, MR: 1.4594, Hits@1: 0.8499, Hits@3: 0.9597\n",
            "Epoch: 64, MRR: 0.8254, MR: 1.8645, Hits@1: 0.7440, Hits@3: 0.8650\n",
            "\n",
            "  Epoch: 65, Loss: 0.9520, Acc: 0.6954\n",
            "Epoch: 65, MRR: 0.9089, MR: 1.4675, Hits@1: 0.8523, Hits@3: 0.9616\n",
            "Epoch: 65, MRR: 0.8283, MR: 1.9646, Hits@1: 0.7532, Hits@3: 0.8727\n",
            "\n",
            "  Epoch: 66, Loss: 0.9310, Acc: 0.7016\n",
            "Epoch: 66, MRR: 0.9091, MR: 1.4672, Hits@1: 0.8524, Hits@3: 0.9617\n",
            "Epoch: 66, MRR: 0.8247, MR: 1.9468, Hits@1: 0.7500, Hits@3: 0.8654\n",
            "\n",
            "  Epoch: 67, Loss: 0.9541, Acc: 0.6937\n",
            "Epoch: 67, MRR: 0.9064, MR: 1.4776, Hits@1: 0.8472, Hits@3: 0.9607\n",
            "Epoch: 67, MRR: 0.8223, MR: 1.9283, Hits@1: 0.7437, Hits@3: 0.8560\n",
            "\n",
            "  Epoch: 68, Loss: 0.9268, Acc: 0.7037\n",
            "Epoch: 68, MRR: 0.9078, MR: 1.4568, Hits@1: 0.8501, Hits@3: 0.9611\n",
            "Epoch: 68, MRR: 0.8164, MR: 1.9232, Hits@1: 0.7318, Hits@3: 0.8559\n",
            "\n",
            "  Epoch: 69, Loss: 0.9337, Acc: 0.7023\n",
            "Epoch: 69, MRR: 0.9084, MR: 1.4668, Hits@1: 0.8521, Hits@3: 0.9592\n",
            "Epoch: 69, MRR: 0.8303, MR: 1.8642, Hits@1: 0.7505, Hits@3: 0.8807\n",
            "\n",
            "  Epoch: 70, Loss: 0.9444, Acc: 0.6973\n",
            "Epoch: 70, MRR: 0.9107, MR: 1.4619, Hits@1: 0.8565, Hits@3: 0.9600\n",
            "Epoch: 70, MRR: 0.8240, MR: 1.9900, Hits@1: 0.7479, Hits@3: 0.8603\n",
            "\n",
            "  Epoch: 71, Loss: 0.9204, Acc: 0.7061\n",
            "Epoch: 71, MRR: 0.9098, MR: 1.4674, Hits@1: 0.8531, Hits@3: 0.9616\n",
            "Epoch: 71, MRR: 0.8249, MR: 1.9594, Hits@1: 0.7498, Hits@3: 0.8674\n",
            "\n",
            "  Epoch: 72, Loss: 0.9088, Acc: 0.7101\n",
            "Epoch: 72, MRR: 0.9106, MR: 1.4560, Hits@1: 0.8550, Hits@3: 0.9617\n",
            "Epoch: 72, MRR: 0.8313, MR: 1.8841, Hits@1: 0.7508, Hits@3: 0.8887\n",
            "\n",
            "  Epoch: 73, Loss: 0.9089, Acc: 0.7070\n",
            "Epoch: 73, MRR: 0.9058, MR: 1.4765, Hits@1: 0.8473, Hits@3: 0.9592\n",
            "Epoch: 73, MRR: 0.8171, MR: 2.0059, Hits@1: 0.7386, Hits@3: 0.8463\n",
            "\n",
            "  Epoch: 74, Loss: 0.9171, Acc: 0.7028\n",
            "Epoch: 74, MRR: 0.9048, MR: 1.4680, Hits@1: 0.8446, Hits@3: 0.9616\n",
            "Epoch: 74, MRR: 0.8232, MR: 1.8902, Hits@1: 0.7420, Hits@3: 0.8589\n",
            "\n",
            "  Epoch: 75, Loss: 0.9120, Acc: 0.7086\n",
            "Epoch: 75, MRR: 0.9092, MR: 1.4558, Hits@1: 0.8514, Hits@3: 0.9629\n",
            "Epoch: 75, MRR: 0.8183, MR: 2.0153, Hits@1: 0.7420, Hits@3: 0.8528\n",
            "\n",
            "  Epoch: 76, Loss: 0.8904, Acc: 0.7169\n",
            "Epoch: 76, MRR: 0.9107, MR: 1.4546, Hits@1: 0.8548, Hits@3: 0.9609\n",
            "Epoch: 76, MRR: 0.8278, MR: 1.8261, Hits@1: 0.7464, Hits@3: 0.8831\n",
            "\n",
            "  Epoch: 77, Loss: 0.9048, Acc: 0.7080\n",
            "Epoch: 77, MRR: 0.9086, MR: 1.4578, Hits@1: 0.8512, Hits@3: 0.9599\n",
            "Epoch: 77, MRR: 0.8288, MR: 1.8888, Hits@1: 0.7530, Hits@3: 0.8661\n",
            "\n",
            "  Epoch: 78, Loss: 0.8820, Acc: 0.7177\n",
            "Epoch: 78, MRR: 0.9084, MR: 1.4607, Hits@1: 0.8507, Hits@3: 0.9602\n",
            "Epoch: 78, MRR: 0.8204, MR: 1.9262, Hits@1: 0.7420, Hits@3: 0.8494\n",
            "\n",
            "  Epoch: 79, Loss: 0.9071, Acc: 0.7062\n",
            "Epoch: 79, MRR: 0.9064, MR: 1.4779, Hits@1: 0.8480, Hits@3: 0.9595\n",
            "Epoch: 79, MRR: 0.8364, MR: 1.8061, Hits@1: 0.7466, Hits@3: 0.9021\n",
            "\n",
            "  Epoch: 80, Loss: 0.9303, Acc: 0.6993\n",
            "Epoch: 80, MRR: 0.9125, MR: 1.4561, Hits@1: 0.8575, Hits@3: 0.9626\n",
            "Epoch: 80, MRR: 0.8267, MR: 1.8599, Hits@1: 0.7488, Hits@3: 0.8550\n",
            "\n",
            "  Epoch: 81, Loss: 0.8856, Acc: 0.7163\n",
            "Epoch: 81, MRR: 0.9079, MR: 1.4597, Hits@1: 0.8489, Hits@3: 0.9629\n",
            "Epoch: 81, MRR: 0.8293, MR: 1.8443, Hits@1: 0.7486, Hits@3: 0.8712\n",
            "\n",
            "  Epoch: 82, Loss: 0.8764, Acc: 0.7194\n",
            "Epoch: 82, MRR: 0.9077, MR: 1.4740, Hits@1: 0.8501, Hits@3: 0.9592\n",
            "Epoch: 82, MRR: 0.8246, MR: 1.8219, Hits@1: 0.7394, Hits@3: 0.8749\n",
            "\n",
            "  Epoch: 83, Loss: 0.8721, Acc: 0.7228\n",
            "Epoch: 83, MRR: 0.9098, MR: 1.4675, Hits@1: 0.8538, Hits@3: 0.9628\n",
            "Epoch: 83, MRR: 0.8180, MR: 1.9398, Hits@1: 0.7357, Hits@3: 0.8555\n",
            "\n",
            "  Epoch: 84, Loss: 0.8828, Acc: 0.7141\n",
            "Epoch: 84, MRR: 0.9084, MR: 1.4595, Hits@1: 0.8518, Hits@3: 0.9585\n",
            "Epoch: 84, MRR: 0.8223, MR: 1.9164, Hits@1: 0.7423, Hits@3: 0.8717\n",
            "\n",
            "  Epoch: 85, Loss: 0.8717, Acc: 0.7218\n",
            "Epoch: 85, MRR: 0.9132, MR: 1.4492, Hits@1: 0.8582, Hits@3: 0.9643\n",
            "Epoch: 85, MRR: 0.8262, MR: 1.8868, Hits@1: 0.7460, Hits@3: 0.8780\n",
            "\n",
            "  Epoch: 86, Loss: 0.8654, Acc: 0.7216\n",
            "Epoch: 86, MRR: 0.9099, MR: 1.4677, Hits@1: 0.8531, Hits@3: 0.9611\n",
            "Epoch: 86, MRR: 0.8215, MR: 1.8912, Hits@1: 0.7377, Hits@3: 0.8576\n",
            "\n",
            "  Epoch: 87, Loss: 0.8647, Acc: 0.7218\n",
            "Epoch: 87, MRR: 0.9100, MR: 1.4515, Hits@1: 0.8535, Hits@3: 0.9634\n",
            "Epoch: 87, MRR: 0.8210, MR: 1.9082, Hits@1: 0.7358, Hits@3: 0.8718\n",
            "\n",
            "  Epoch: 88, Loss: 0.8538, Acc: 0.7255\n",
            "Epoch: 88, MRR: 0.9161, MR: 1.4498, Hits@1: 0.8631, Hits@3: 0.9634\n",
            "Epoch: 88, MRR: 0.8245, MR: 1.9346, Hits@1: 0.7476, Hits@3: 0.8518\n",
            "\n",
            "  Epoch: 89, Loss: 0.8554, Acc: 0.7189\n",
            "Epoch: 89, MRR: 0.9117, MR: 1.4493, Hits@1: 0.8550, Hits@3: 0.9655\n",
            "Epoch: 89, MRR: 0.8328, MR: 1.7858, Hits@1: 0.7476, Hits@3: 0.8939\n",
            "\n",
            "  Epoch: 90, Loss: 0.8471, Acc: 0.7264\n",
            "Epoch: 90, MRR: 0.9108, MR: 1.4480, Hits@1: 0.8553, Hits@3: 0.9624\n",
            "Epoch: 90, MRR: 0.8220, MR: 1.9216, Hits@1: 0.7392, Hits@3: 0.8768\n",
            "\n",
            "  Epoch: 91, Loss: 0.8831, Acc: 0.7137\n",
            "Epoch: 91, MRR: 0.9061, MR: 1.4526, Hits@1: 0.8450, Hits@3: 0.9626\n",
            "Epoch: 91, MRR: 0.8170, MR: 1.9119, Hits@1: 0.7301, Hits@3: 0.8756\n",
            "\n",
            "  Epoch: 92, Loss: 0.8731, Acc: 0.7190\n",
            "Epoch: 92, MRR: 0.9121, MR: 1.4590, Hits@1: 0.8572, Hits@3: 0.9623\n",
            "Epoch: 92, MRR: 0.8162, MR: 2.0352, Hits@1: 0.7372, Hits@3: 0.8552\n",
            "\n",
            "  Epoch: 93, Loss: 0.8581, Acc: 0.7265\n",
            "Epoch: 93, MRR: 0.9127, MR: 1.4538, Hits@1: 0.8580, Hits@3: 0.9616\n",
            "Epoch: 93, MRR: 0.8277, MR: 1.9187, Hits@1: 0.7510, Hits@3: 0.8771\n",
            "\n",
            "  Epoch: 94, Loss: 0.8625, Acc: 0.7223\n",
            "Epoch: 94, MRR: 0.9132, MR: 1.4493, Hits@1: 0.8599, Hits@3: 0.9614\n",
            "Epoch: 94, MRR: 0.8216, MR: 1.9908, Hits@1: 0.7476, Hits@3: 0.8560\n",
            "\n",
            "  Epoch: 95, Loss: 0.8699, Acc: 0.7207\n",
            "Epoch: 95, MRR: 0.9105, MR: 1.4612, Hits@1: 0.8553, Hits@3: 0.9595\n",
            "Epoch: 95, MRR: 0.8237, MR: 1.9162, Hits@1: 0.7477, Hits@3: 0.8604\n",
            "\n",
            "  Epoch: 96, Loss: 0.8597, Acc: 0.7201\n",
            "Epoch: 96, MRR: 0.9142, MR: 1.4371, Hits@1: 0.8596, Hits@3: 0.9645\n",
            "Epoch: 96, MRR: 0.8290, MR: 1.8530, Hits@1: 0.7515, Hits@3: 0.8710\n",
            "\n",
            "  Epoch: 97, Loss: 0.8359, Acc: 0.7284\n",
            "Epoch: 97, MRR: 0.9090, MR: 1.4529, Hits@1: 0.8514, Hits@3: 0.9614\n",
            "Epoch: 97, MRR: 0.8297, MR: 1.8433, Hits@1: 0.7498, Hits@3: 0.8686\n",
            "\n",
            "  Epoch: 98, Loss: 0.8173, Acc: 0.7368\n",
            "Epoch: 98, MRR: 0.9127, MR: 1.4560, Hits@1: 0.8580, Hits@3: 0.9623\n",
            "Epoch: 98, MRR: 0.8269, MR: 1.9043, Hits@1: 0.7501, Hits@3: 0.8543\n",
            "\n",
            "  Epoch: 99, Loss: 0.8273, Acc: 0.7332\n",
            "Epoch: 99, MRR: 0.9121, MR: 1.4643, Hits@1: 0.8577, Hits@3: 0.9619\n",
            "Epoch: 99, MRR: 0.8268, MR: 1.9153, Hits@1: 0.7474, Hits@3: 0.8706\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUj0FaKXkrLg",
        "outputId": "7fb24f2e-1313-4b33-8e69-9fc231b76fdd"
      },
      "source": [
        "m = nn.LogSoftmax(dim=1)\n",
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "target = torch.tensor([1, 0, 4])\n",
        "logs = m(input)\n",
        "\n",
        "print(target)\n",
        "print(logs)\n",
        "      \n",
        "# tensor([1, 0, 4])\n",
        "# tensor([[-1.6392, -2.3468, -1.0096, -1.4705, -2.1542], 4\n",
        "#         [-2.8297, -1.4175, -2.2059, -1.4539, -1.0361], 3\n",
        "#         [-3.0506, -1.9580, -0.7626, -1.2412, -2.8820]] 3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 0, 4])\n",
            "tensor([[-0.6964, -2.1560, -4.0682, -1.8711, -1.5381],\n",
            "        [-2.4510, -2.4992, -0.9120, -0.9522, -3.1229],\n",
            "        [-1.3784, -2.1717, -2.7236, -0.8271, -2.0318]],\n",
            "       grad_fn=<LogSoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-ClSIoDoefl",
        "outputId": "993a136a-e741-40c8-8b1c-c4b797a16d00"
      },
      "source": [
        "_, perm = logs.sort(dim=1, descending=True)\n",
        "mask = (target.view(-1, 1) == perm)\n",
        "nnz = mask.nonzero(as_tuple=False)\n",
        "\n",
        "print(perm)\n",
        "print(target.view(-1, 1) == perm)\n",
        "print(mask.nonzero(as_tuple=False))\n",
        "print((nnz[:, -1] + 1).to(torch.float).mean().item())\n",
        "\n",
        "# mask = (y.view(-1, 1) == perm)\n",
        "\n",
        "# nnz = mask.nonzero(as_tuple=False)\n",
        "# mrr = (1 / (nnz[:, -1] + 1).to(torch.float)).mean().item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 4, 3, 1, 2],\n",
            "        [2, 3, 0, 1, 4],\n",
            "        [3, 0, 4, 1, 2]])\n",
            "tensor([[False, False, False,  True, False],\n",
            "        [False, False,  True, False, False],\n",
            "        [False, False,  True, False, False]])\n",
            "tensor([[0, 3],\n",
            "        [1, 2],\n",
            "        [2, 2]])\n",
            "3.3333332538604736\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKKe61IQpBZL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}